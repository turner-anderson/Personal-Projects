{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno\n",
    "import logging\n",
    "import time\n",
    "import getpass\n",
    "from tqdm import tqdm_notebook\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchFrameException\n",
    "from address_scraping import *\n",
    "\n",
    "cp = fr'C:\\Users\\{getpass.getuser()}\\AppData\\Local\\Microsoft\\WindowsApps\\chromedriver.exe'\n",
    "%matplotlib inline\n",
    "\n",
    "## Set up logger\n",
    "logging.basicConfig(\n",
    "                    format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',\n",
    "                    datefmt='%m-%d %H:%M',\n",
    "                    )\n",
    "_log = logging.getLogger(name=__name__)\n",
    "_log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dataframe\n",
    "p = r'D:\\LoveAkron\\Orgs, Households, Addresses by Donation-2020-02-06-14-43-49.xlsx'\n",
    "df = pd.read_excel(p, dtype=str)\n",
    "display(df.head(5))\n",
    "display(missingno.matrix(df)) # Examine how much missing data we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop duplicate names, since we can easily map back to them later\n",
    "df['full_name'] = df['First Name'] + ' ' + df['Last Name']\n",
    "df['counts'] = df['full_name'].map(df['full_name'].value_counts())\n",
    "_log.debug(f\"Pre-drop we had {df.shape[0]} observations\")\n",
    "df.drop_duplicates(subset=['full_name'], inplace=True)\n",
    "_log.debug(f\"Post-drop we had {df.shape[0]} observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open a browser object\n",
    "browser = webdriver.Chrome(executable_path=cp)\n",
    "\n",
    "# Set the out path for HTML files\n",
    "out_p = r'D:\\LoveAkron\\web_data\\housing\\\\'\n",
    "\n",
    "# Set up final data frame, and instantiate an iterator for scraping\n",
    "iterator = df.iterrows()\n",
    "final_data = pd.DataFrame(columns=['first_name', 'last_name', 'lives_here', 'address', 'city', 'zipcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in tqdm_notebook(iterator, desc='Scraping in Progress:', total=df.shape[0]):\n",
    "    fn = row[1]['First Name']\n",
    "    ln = row[1]['Last Name']\n",
    "    _log.debug(f'Current individual: {fn} {ln}')\n",
    "    if fn and ln:\n",
    "        row_data = scrape_addresses(fn, ln, browser, out_p=out_p)\n",
    "        if row_data:\n",
    "            final_data.loc[i] = row_data\n",
    "        # Sleep so as to not overburden the site\n",
    "        time.sleep(10 + np.random.normal(12,3))\n",
    "\n",
    "# Export final_data to a CSV\n",
    "final_data.to_csv(f'{out_p}full_data.csv', index=False, sep='|')\n",
    "\n",
    "# Clean up\n",
    "browser.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
